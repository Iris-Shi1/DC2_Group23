{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    ""
   ],
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T17:20:37.406598Z",
     "start_time": "2024-06-10T17:20:28.222439Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/PAS+crime+extra_final.csv')\n",
    "\n",
    "# Example mapping of borough numbers to names\n",
    "borough_mapping = {\n",
    "    1: 'Barking and Dagenham', 2: 'Barnet', 3: 'Bexley', 4: 'Brent', 5: 'Bromley',\n",
    "    6: 'Camden', 7: 'Croydon', 8: 'Ealing', 9: 'Enfield', 10: 'Greenwich',\n",
    "    11: 'Hackney', 12: 'Hammersmith and Fulham', 13: 'Haringey', 14: 'Harrow',\n",
    "    15: 'Havering', 16: 'Hillingdon', 17: 'Hounslow', 18: 'Islington',\n",
    "    19: 'Kensington and Chelsea', 20: 'Kingston upon Thames', 21: 'Lambeth',\n",
    "    22: 'Lewisham', 23: 'Merton', 24: 'Newham', 25: 'Redbridge', 26: 'Richmond upon Thames',\n",
    "    27: 'Southwark', 28: 'Sutton', 29: 'Tower Hamlets', 30: 'Waltham Forest',\n",
    "    31: 'Wandsworth', 32: 'Westminster'\n",
    "}\n",
    "\n",
    "# Map the borough numbers to names\n",
    "data['Borough_Name'] = data['Borough'].map(borough_mapping)\n",
    "\n",
    "# Aggregate shoplifting instances by borough\n",
    "agg_data = data.groupby('Borough_Name')['Shoplifting'].sum().reset_index()\n",
    "\n",
    "# Load a GeoDataFrame for London boroughs from the GeoPackage\n",
    "gdf = gpd.read_file('../project_data/London_Boroughs.gpkg')\n",
    "\n",
    "# Merge aggregated data with GeoDataFrame using 'name' as the key\n",
    "gdf = gdf.merge(agg_data, left_on='name', right_on='Borough_Name', how='left')\n",
    "\n",
    "# Create a Folium map\n",
    "m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "# Add boroughs to the map with color representing shoplifting instances\n",
    "folium.Choropleth(\n",
    "    geo_data=gdf,\n",
    "    name='choropleth',\n",
    "    data=gdf,\n",
    "    columns=['name', 'Shoplifting'],\n",
    "    key_on='feature.properties.name',\n",
    "    fill_color='YlOrRd',\n",
    "    fill_opacity=0.7,\n",
    "    line_opacity=0.2,\n",
    "    legend_name='Shoplifting Instances'\n",
    ").add_to(m)\n",
    "\n",
    "# Add layer control\n",
    "folium.LayerControl().add_to(m)\n",
    "\n",
    "# Save the map to an HTML file\n",
    "m.save('london_shoplifting.html')\n",
    "\n",
    "# Display the map inline (optional, for Jupyter Notebooks)\n",
    "m\n"
   ],
   "id": "75f69210ea28a57c",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-10T17:19:07.644424Z",
     "start_time": "2024-06-10T17:19:00.671974Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/PAS+crime+extra_final.csv')\n",
    "\n",
    "# Example mapping of borough numbers to names\n",
    "borough_mapping = {\n",
    "    1: 'Barking and Dagenham', 2: 'Barnet', 3: 'Bexley', 4: 'Brent', 5: 'Bromley',\n",
    "    6: 'Camden', 7: 'Croydon', 8: 'Ealing', 9: 'Enfield', 10: 'Greenwich',\n",
    "    11: 'Hackney', 12: 'Hammersmith and Fulham', 13: 'Haringey', 14: 'Harrow',\n",
    "    15: 'Havering', 16: 'Hillingdon', 17: 'Hounslow', 18: 'Islington',\n",
    "    19: 'Kensington and Chelsea', 20: 'Kingston upon Thames', 21: 'Lambeth',\n",
    "    22: 'Lewisham', 23: 'Merton', 24: 'Newham', 25: 'Redbridge', 26: 'Richmond upon Thames',\n",
    "    27: 'Southwark', 28: 'Sutton', 29: 'Tower Hamlets', 30: 'Waltham Forest',\n",
    "    31: 'Wandsworth', 32: 'Westminster'\n",
    "}\n",
    "\n",
    "# Map the borough numbers to names\n",
    "data['Borough_Name'] = data['Borough'].map(borough_mapping)\n",
    "\n",
    "# Aggregate shoplifting instances by borough\n",
    "agg_data = data.groupby('Borough_Name')['Shoplifting'].sum().reset_index()\n",
    "\n",
    "# Load a GeoDataFrame for London boroughs from the GeoPackage\n",
    "gdf = gpd.read_file('project_data/London_Boroughs.gpkg')\n",
    "\n",
    "# Inspect the columns to find the correct column name for boroughs\n",
    "print(gdf.columns)\n"
   ],
   "id": "75c20a751b9c771e",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:53:42.731753Z",
     "start_time": "2024-06-11T18:51:44.567261Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/combined_crime.csv', low_memory=False)\n",
    "\n",
    "# Check the first few rows of data to ensure it loaded correctly\n",
    "print(\"Loaded Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Filter for shoplifting instances\n",
    "shoplifting_data = data[data['Crime type'].str.strip().str.lower() == 'shoplifting']\n",
    "\n",
    "# Check the filtered data\n",
    "print(\"Filtered Data (Shoplifting):\")\n",
    "print(shoplifting_data.head())\n",
    "\n",
    "# Ensure that the filtered data is not empty\n",
    "if shoplifting_data.empty:\n",
    "    print(\"No shoplifting data found after filtering.\")\n",
    "else:\n",
    "    # Aggregate data by location\n",
    "    agg_data = shoplifting_data.groupby(['Latitude', 'Longitude']).size().reset_index(name='count')\n",
    "\n",
    "    # Check the aggregated data\n",
    "    print(\"Aggregated Data:\")\n",
    "    print(agg_data.head())\n",
    "\n",
    "    # Check for missing or invalid coordinates\n",
    "    if agg_data[['Latitude', 'Longitude']].isnull().values.any():\n",
    "        print(\"Missing coordinates detected\")\n",
    "    else:\n",
    "        print(\"No missing coordinates\")\n",
    "\n",
    "        # Create a Folium map\n",
    "        m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "        # Create a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add shoplifting points to the cluster\n",
    "        for idx, row in agg_data.iterrows():\n",
    "            if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude']):\n",
    "                folium.CircleMarker(\n",
    "                    location=(row['Latitude'], row['Longitude']),\n",
    "                    radius=5 + row['count'],  # Increase radius based on the number of cases\n",
    "                    color='red',\n",
    "                    fill=True,\n",
    "                    fill_color='red',\n",
    "                    popup=f\"Shoplifting cases: {row['count']}\"\n",
    "                ).add_to(marker_cluster)\n",
    "\n",
    "        # Optionally, add a heatmap for density visualization\n",
    "        heat_data = [[row['Latitude'], row['Longitude']] for index, row in shoplifting_data.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])]\n",
    "        HeatMap(heat_data).add_to(m)\n",
    "\n",
    "        # Save the map to an HTML file\n",
    "        m.save('shoplifting_heatmap.html')\n",
    "\n",
    "        # Display the map inline (optional, for Jupyter Notebooks)\n",
    "        m\n",
    "\n",
    "print(\"Map creation process complete.\")\n"
   ],
   "id": "97dd83290c6c5f19",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T18:58:15.381710Z",
     "start_time": "2024-06-11T18:56:25.424612Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/combined_crime.csv', low_memory=False)\n",
    "\n",
    "# Check the first few rows of data to ensure it loaded correctly\n",
    "print(\"Loaded Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Filter for shoplifting instances\n",
    "shoplifting_data = data[data['Crime type'].str.strip().str.lower() == 'shoplifting']\n",
    "\n",
    "# Check the filtered data\n",
    "print(\"Filtered Data (Shoplifting):\")\n",
    "print(shoplifting_data.head())\n",
    "\n",
    "# Ensure that the filtered data is not empty\n",
    "if shoplifting_data.empty:\n",
    "    print(\"No shoplifting data found after filtering.\")\n",
    "else:\n",
    "    # Aggregate data by location\n",
    "    agg_data = shoplifting_data.groupby(['Latitude', 'Longitude']).size().reset_index(name='count')\n",
    "\n",
    "    # Check the aggregated data\n",
    "    print(\"Aggregated Data:\")\n",
    "    print(agg_data.head())\n",
    "\n",
    "    # Check for missing or invalid coordinates\n",
    "    if agg_data[['Latitude', 'Longitude']].isnull().values.any():\n",
    "        print(\"Missing coordinates detected\")\n",
    "    else:\n",
    "        print(\"No missing coordinates\")\n",
    "\n",
    "        # Create a Folium map\n",
    "        m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "        # Create a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add shoplifting points to the cluster with a blue color scheme\n",
    "        for idx, row in agg_data.iterrows():\n",
    "            if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude']):\n",
    "                folium.CircleMarker(\n",
    "                    location=(row['Latitude'], row['Longitude']),\n",
    "                    radius=5 + row['count'],  # Increase radius based on the number of cases\n",
    "                    color='blue',\n",
    "                    fill=True,\n",
    "                    fill_color='blue',\n",
    "                    popup=f\"Shoplifting cases: {row['count']}\"\n",
    "                ).add_to(marker_cluster)\n",
    "\n",
    "        # Add a heatmap for density visualization with a new color gradient\n",
    "        heat_data = [[row['Latitude'], row['Longitude']] for index, row in shoplifting_data.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])]\n",
    "        HeatMap(heat_data, gradient={0.2: 'yellow', 0.4: 'orange', 0.6: 'red', 0.8: 'darkred', 1: 'black'}).add_to(m)\n",
    "\n",
    "        # Save the map to an HTML file\n",
    "        m.save('shoplifting_heatmap_yellow_black.html')\n",
    "\n",
    "        # Display the map inline (optional, for Jupyter Notebooks)\n",
    "        m\n",
    "\n",
    "print(\"Map creation process complete.\")\n"
   ],
   "id": "dffdda6f0fe82095",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:12:18.324710Z",
     "start_time": "2024-06-11T19:10:34.209574Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import folium\n",
    "from folium.plugins import MarkerCluster, HeatMap\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/combined_crime.csv', low_memory=False)\n",
    "\n",
    "# Check the first few rows of data to ensure it loaded correctly\n",
    "print(\"Loaded Data:\")\n",
    "print(data.head())\n",
    "\n",
    "# Filter for shoplifting instances\n",
    "shoplifting_data = data[data['Crime type'].str.strip().str.lower() == 'shoplifting']\n",
    "\n",
    "# Check the filtered data\n",
    "print(\"Filtered Data (Shoplifting):\")\n",
    "print(shoplifting_data.head())\n",
    "\n",
    "# Ensure that the filtered data is not empty\n",
    "if shoplifting_data.empty:\n",
    "    print(\"No shoplifting data found after filtering.\")\n",
    "else:\n",
    "    # Aggregate data by location\n",
    "    agg_data = shoplifting_data.groupby(['Latitude', 'Longitude']).size().reset_index(name='count')\n",
    "\n",
    "    # Check the aggregated data\n",
    "    print(\"Aggregated Data:\")\n",
    "    print(agg_data.head())\n",
    "\n",
    "    # Check for missing or invalid coordinates\n",
    "    if agg_data[['Latitude', 'Longitude']].isnull().values.any():\n",
    "        print(\"Missing coordinates detected\")\n",
    "    else:\n",
    "        print(\"No missing coordinates\")\n",
    "\n",
    "        # Create a Folium map\n",
    "        m = folium.Map(location=[51.5074, -0.1278], zoom_start=10)\n",
    "\n",
    "        # Create a marker cluster\n",
    "        marker_cluster = MarkerCluster().add_to(m)\n",
    "\n",
    "        # Add shoplifting points to the cluster with a green color scheme and lower opacity\n",
    "        for idx, row in agg_data.iterrows():\n",
    "            if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude']):\n",
    "                folium.CircleMarker(\n",
    "                    location=(row['Latitude'], row['Longitude']),\n",
    "                    radius=5 + row['count'],  # Increase radius based on the number of cases\n",
    "                    color='green',\n",
    "                    fill=True,\n",
    "                    fill_color='green',\n",
    "                    fill_opacity=0.5,  # Set lower fill opacity\n",
    "                    popup=f\"Shoplifting cases: {row['count']}\"\n",
    "                ).add_to(marker_cluster)\n",
    "\n",
    "        # Add a heatmap for density visualization with a blue color gradient and lower opacity\n",
    "        heat_data = [[row['Latitude'], row['Longitude']] for index, row in shoplifting_data.iterrows() if pd.notnull(row['Latitude']) and pd.notnull(row['Longitude'])]\n",
    "        HeatMap(heat_data, gradient={0.2: 'lightblue', 0.4: 'skyblue', 0.6: 'dodgerblue', 0.8: 'blue', 1: 'darkblue'}, radius=15, blur=10, max_opacity=0.5).add_to(m)\n",
    "\n",
    "        # Save the map to an HTML file\n",
    "        m.save('shoplifting_heatmap.html')\n",
    "\n",
    "        # Display the map inline (optional, for Jupyter Notebooks)\n",
    "        m\n",
    "\n",
    "print(\"Map creation process complete.\")\n"
   ],
   "id": "7485cab880d05bcc",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "from dash import html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/combined_crime.csv', low_memory=False)\n",
    "\n",
    "# Extract year from the 'Month' column\n",
    "data['Year'] = pd.to_datetime(data['Month'], format='%Y-%m').dt.year\n",
    "\n",
    "# Filter for shoplifting instances\n",
    "shoplifting_data = data[data['Crime type'].str.strip().str.lower() == 'shoplifting']\n",
    "\n",
    "# Initialize the Dash app\n",
    "app = dash.Dash(__name__, external_stylesheets=['https://codepen.io/chriddyp/pen/bWLwgP.css'])\n",
    "\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"Shoplifting Incidents in London\"),\n",
    "    dcc.Dropdown(\n",
    "        id='year-dropdown',\n",
    "        options=[{'label': str(year), 'value': year} for year in sorted(shoplifting_data['Year'].unique())],\n",
    "        value=sorted(shoplifting_data['Year'].unique())[0],\n",
    "        clearable=False\n",
    "    ),\n",
    "    dcc.Graph(id='map-graph')\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('map-graph', 'figure'),\n",
    "    Input('year-dropdown', 'value')\n",
    ")\n",
    "def update_map(selected_year):\n",
    "    filtered_data = shoplifting_data[shoplifting_data['Year'] == selected_year]\n",
    "\n",
    "    # Aggregate data by location\n",
    "    agg_data = filtered_data.groupby(['Latitude', 'Longitude']).size().reset_index(name='count')\n",
    "\n",
    "    fig = px.scatter_mapbox(\n",
    "        agg_data,\n",
    "        lat='Latitude',\n",
    "        lon='Longitude',\n",
    "        size='count',\n",
    "        color_continuous_scale='Viridis',\n",
    "        size_max=15,\n",
    "        zoom=10,\n",
    "        mapbox_style=\"carto-positron\",\n",
    "        title=f\"Shoplifting Incidents in {selected_year}\",\n",
    "        labels={'count': 'Number of Incidents'}\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)\n"
   ],
   "id": "29e64702e0af4a8d",
   "execution_count": null,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:42:45.094216Z",
     "start_time": "2024-06-17T18:42:35.083081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# RUN\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/PAS+crime+extra_final.csv')\n",
    "\n",
    "# Example mapping of borough numbers to names\n",
    "borough_mapping = {\n",
    "    1: 'Barking and Dagenham', 2: 'Barnet', 3: 'Bexley', 4: 'Brent', 5: 'Bromley',\n",
    "    6: 'Camden', 7: 'Croydon', 8: 'Ealing', 9: 'Enfield', 10: 'Greenwich',\n",
    "    11: 'Hackney', 12: 'Hammersmith and Fulham', 13: 'Haringey', 14: 'Harrow',\n",
    "    15: 'Havering', 16: 'Hillingdon', 17: 'Hounslow', 18: 'Islington',\n",
    "    19: 'Kensington and Chelsea', 20: 'Kingston upon Thames', 21: 'Lambeth',\n",
    "    22: 'Lewisham', 23: 'Merton', 24: 'Newham', 25: 'Redbridge', 26: 'Richmond upon Thames',\n",
    "    27: 'Southwark', 28: 'Sutton', 29: 'Tower Hamlets', 30: 'Waltham Forest',\n",
    "    31: 'Wandsworth', 32: 'Westminster'\n",
    "}\n",
    "\n",
    "# Map the borough numbers to names\n",
    "data['Borough_Name'] = data['Borough'].map(borough_mapping)\n",
    "\n",
    "# Aggregate shoplifting instances by borough\n",
    "agg_data = data.groupby('Borough_Name')['Shoplifting'].sum().reset_index()\n",
    "\n",
    "# Load a GeoDataFrame for London boroughs from the GeoPackage\n",
    "gdf = gpd.read_file('../project_data/London_Boroughs.gpkg')\n",
    "\n",
    "# Merge the importance scores with the geographical information of London boroughs\n",
    "merged_data = gdf.merge(agg_data, how='left', left_on='name', right_on='Borough_Name')\n",
    "\n",
    "# Plot the spatial data with importance scores using a reversed color gradient\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "merged_data.plot(column='Shoplifting', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "ax.set_title('Frequency of Shoplifting in Different London Boroughs')\n",
    "plt.savefig('Shoplifting_figure.png')\n",
    "plt.show()\n"
   ],
   "id": "514d99db3661785d",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-11T19:57:34.136457Z",
     "start_time": "2024-06-11T19:57:34.124846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# RUN\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "data = pd.read_csv('../project_data/PAS+crime+extra_final.csv')\n",
    "\n",
    "# Example mapping of borough numbers to names\n",
    "borough_mapping = {\n",
    "    1: 'Barking and Dagenham', 2: 'Barnet', 3: 'Bexley', 4: 'Brent', 5: 'Bromley',\n",
    "    6: 'Camden', 7: 'Croydon', 8: 'Ealing', 9: 'Enfield', 10: 'Greenwich',\n",
    "    11: 'Hackney', 12: 'Hammersmith and Fulham', 13: 'Haringey', 14: 'Harrow',\n",
    "    15: 'Havering', 16: 'Hillingdon', 17: 'Hounslow', 18: 'Islington',\n",
    "    19: 'Kensington and Chelsea', 20: 'Kingston upon Thames', 21: 'Lambeth',\n",
    "    22: 'Lewisham', 23: 'Merton', 24: 'Newham', 25: 'Redbridge', 26: 'Richmond upon Thames',\n",
    "    27: 'Southwark', 28: 'Sutton', 29: 'Tower Hamlets', 30: 'Waltham Forest',\n",
    "    31: 'Wandsworth', 32: 'Westminster'\n",
    "}\n",
    "\n",
    "# Map the borough numbers to names\n",
    "data['Borough_Name'] = data['Borough'].map(borough_mapping)\n",
    "\n",
    "# Aggregate shoplifting instances by borough\n",
    "agg_data = data.groupby('Borough_Name')['Shoplifting'].sum().reset_index()\n",
    "\n",
    "# Load a GeoDataFrame for London boroughs from the GeoPackage\n",
    "gdf = gpd.read_file('project_data/London_Boroughs.gpkg')\n",
    "\n",
    "# Merge the importance scores with the geographical information of London boroughs\n",
    "merged_data = gdf.merge(agg_data, how='left', left_on='name', right_on='Borough_Name')\n",
    "\n",
    "# Calculate the natural logarithm of shoplifting instances\n",
    "merged_data['Shoplifting_Log'] = np.log(merged_data['Shoplifting'])\n",
    "\n",
    "# Plot the spatial data with logged importance scores using a reversed color gradient\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "merged_data.plot(column='Shoplifting_Log', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "ax.set_title('Log Frequency of Shoplifting in Different London Boroughs')\n",
    "plt.savefig('Shoplifting_log_figure.png')\n",
    "plt.show()\n"
   ],
   "id": "bcf906c7eb9d330e",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T18:52:12.902449Z",
     "start_time": "2024-06-17T18:52:11.936103Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#RUN\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Provided dictionary\n",
    "borough_data = {\n",
    "    \"Islington\": 74,\n",
    "    \"Sutton\": 88,\n",
    "    \"Bromley\": 122,\n",
    "    \"Lewisham\": 0,\n",
    "    \"Bexley\": 0,\n",
    "    \"Haringey\": 56,\n",
    "    \"Harrow\": 83,\n",
    "    \"Enfield\": 0,\n",
    "    \"Croydon\": 88,\n",
    "    \"Newham\": 95,\n",
    "    \"Barking and Dagenham\": 70,\n",
    "    \"Havering\": \"54/74\",\n",
    "    \"Redbridge\": 53,\n",
    "    \"Southwark\": 34,\n",
    "    \"Lambeth\": 0,\n",
    "    \"Hounslow\": 0,\n",
    "    \"Greenwich\": 0,\n",
    "    \"Kingston upon Thames\": 0,\n",
    "    \"Merton\": 0,\n",
    "    \"Barnet\": 58,\n",
    "    \"Richmond upon Thames\": 49,\n",
    "    \"Hillingdon\": 0,\n",
    "    \"Brent\": 0,\n",
    "    \"Ealing\": 71,\n",
    "    \"Wandsworth\": 48,\n",
    "    \"Tower Hamlets\": 0,\n",
    "    \"Hammersmith and Fulham\": 0,\n",
    "    \"Westminster\": 79,\n",
    "    \"Kensington and Chelsea\": 0,\n",
    "    \"Waltham Forest\": 51,\n",
    "    \"Hackney\": 0,\n",
    "    \"Camden\": 0\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "borough_df = pd.DataFrame(list(borough_data.items()), columns=['Borough_Name', 'Value'])\n",
    "\n",
    "# Convert \"54/74\" to the average of 54 and 74\n",
    "borough_df['Value'] = borough_df['Value'].apply(lambda x: sum(map(int, x.split('/'))) / 2 if isinstance(x, str) and '/' in x else x).astype(float)\n",
    "\n",
    "# Load a GeoDataFrame for London boroughs from the GeoPackage\n",
    "gdf = gpd.read_file('../project_data/London_Boroughs.gpkg')\n",
    "\n",
    "# Merge the borough data with the geographical information of London boroughs\n",
    "merged_data = gdf.merge(borough_df, how='left', left_on='name', right_on='Borough_Name')\n",
    "\n",
    "# Plot the spatial data with values from the dictionary using a color gradient\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "merged_data.plot(column='Value', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "ax.set_title('Importance Values for Shoplifiting by Borough in London')\n",
    "plt.savefig('Importance_values_shoplifting.png')\n",
    "plt.show()\n"
   ],
   "id": "71894819d07ecf86",
   "execution_count": 7,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-17T19:03:27.688025Z",
     "start_time": "2024-06-17T19:03:25.920232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#RUN\n",
    "\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Provided dictionary\n",
    "borough_data = {\n",
    "    \"Islington\": 74,\n",
    "    \"Sutton\": 88,\n",
    "    \"Bromley\": 122,\n",
    "    \"Lewisham\": 0,\n",
    "    \"Bexley\": 0,\n",
    "    \"Haringey\": 56,\n",
    "    \"Harrow\": 83,\n",
    "    \"Enfield\": 0,\n",
    "    \"Croydon\": 88,\n",
    "    \"Newham\": 95,\n",
    "    \"Barking and Dagenham\": 70,\n",
    "    \"Havering\": \"54/74\",\n",
    "    \"Redbridge\": 53,\n",
    "    \"Southwark\": 34,\n",
    "    \"Lambeth\": 0,\n",
    "    \"Hounslow\": 0,\n",
    "    \"Greenwich\": 0,\n",
    "    \"Kingston upon Thames\": 0,\n",
    "    \"Merton\": 0,\n",
    "    \"Barnet\": 58,\n",
    "    \"Richmond upon Thames\": 49,\n",
    "    \"Hillingdon\": 0,\n",
    "    \"Brent\": 0,\n",
    "    \"Ealing\": 71,\n",
    "    \"Wandsworth\": 48,\n",
    "    \"Tower Hamlets\": 0,\n",
    "    \"Hammersmith and Fulham\": 0,\n",
    "    \"Westminster\": 79,\n",
    "    \"Kensington and Chelsea\": 0,\n",
    "    \"Waltham Forest\": 51,\n",
    "    \"Hackney\": 0,\n",
    "    \"Camden\": 0\n",
    "}\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "borough_df = pd.DataFrame(list(borough_data.items()), columns=['Borough_Name', 'Value'])\n",
    "\n",
    "# Convert \"54/74\" to the average of 54 and 74\n",
    "borough_df['Value'] = borough_df['Value'].apply(lambda x: sum(map(int, x.split('/'))) / 2 if isinstance(x, str) and '/' in x else x).astype(float)\n",
    "\n",
    "# Apply a logarithmic transformation to the values\n",
    "borough_df['Logged_Value'] = np.log1p(borough_df['Value'])  # Logarithm with base e (natural logarithm)\n",
    "\n",
    "# Load a GeoDataFrame for London boroughs from the GeoPackage\n",
    "gdf = gpd.read_file('../project_data/London_Boroughs.gpkg')\n",
    "\n",
    "# Merge the borough data with the geographical information of London boroughs\n",
    "merged_data = gdf.merge(borough_df, how='left', left_on='name', right_on='Borough_Name')\n",
    "\n",
    "# Plot the spatial data with logged values using a color gradient\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "merged_data.plot(column='Logged_Value', cmap='Blues', linewidth=0.8, ax=ax, edgecolor='0.8', legend=True)\n",
    "ax.set_title('Logged Importance Values for Shoplifting by Borough in London')\n",
    "plt.savefig('Logged_importance_values_shoplifting.png')\n",
    "plt.show()\n"
   ],
   "id": "5a4c32e2a04626b0",
   "execution_count": 8,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
